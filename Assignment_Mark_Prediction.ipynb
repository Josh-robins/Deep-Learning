{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMvvLj5WXtmnRCzcZzk6ZQ0",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Josh-robins/Deep-Learning/blob/main/Assignment_Mark_Prediction.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Introduction**"
      ],
      "metadata": {
        "id": "akkqF8ksqReT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This notebook implements an **assignment marker** that classifies student marks into binary categories: pass (1) or fail (0).\n",
        "\n",
        "The dataset consists of assignment scores and corresponding labels, which are used to train a simple neural network. While this task could be efficiently solved using basic conditional logic (e.g., an `if-else` statement setting a pass threshold at 75), the goal here is educational: to explore deep learning concepts and gain hands-on experience with PyTorch.,\n",
        "        \n",
        "By leveraging PyTorch, we define a neural network, preprocess the data, train the model, and evaluate its performance. This over-engineered approach serves as a practical introduction to key deep learning components—such as datasets, data loaders, neural network architecture, loss functions, and optimizers—while demonstrating how these tools can be applied to even a simple classification problem.\n",
        "\n",
        "    \n",
        "The notebook is structured as follows\n",
        "1. Importing libraries\n",
        "2. Defining the dataset and data loaders\n",
        "3. Scaling the data\n",
        "4. Building the neural network model\n",
        "5. Setting up the loss function and optimizer\n",
        "6. Training the model\n",
        "7. Evaluating the results\n",
        "        \n",
        "This project highlights the flexibility of PyTorch and provides a foundation for tackling more complex machine learning tasks."
      ],
      "metadata": {
        "id": "Ez05RjJGp9_t"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**1. Importing Libraries**"
      ],
      "metadata": {
        "id": "gpj2I84WeHar"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "4AmEOymneLtj"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Assignment marks: training set\n",
        "train_samples_np = np.array([78, 100, 52, 89, 92, 87, 65, 40, 78, 82, 64, 78, 98, 86, 72, 81, 94, 92, 51, 71])\n",
        "train_labels_np = np.array([  1,   1,  0,  1,  1,  1,  0,  0,  1,  1,  0,  1,  1,  1,  0,  1,  1,  1,  0,  0])\n",
        "\n",
        "# Assignment marks: testing set\n",
        "test_samples_np = np.array([75, 68, 99, 82, 71, 70, 68, 84, 87, 72, 61, 92, 93, 54, 63, 45, 74, 76, 83, 91])\n",
        "test_labels_np = np.array([  1,  0,  1,  1,  0,  0,  0,  1,  1,  0,  0,  1,  1,  0,  0,  0,  0,  1,  1,  1])"
      ],
      "metadata": {
        "id": "LNrSUqtKeUW-"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # print(train_samples_np)\n",
        "# for grade in train_samples_np:\n",
        "#     if grade >= 75:\n",
        "#         print(f'{grade}:1')\n",
        "#     else:\n",
        "#         print(f'{grade}:0')\n",
        "\n",
        "#     # print(grade)"
      ],
      "metadata": {
        "id": "f6WBWnGNeghM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**2. Define the Data loader**"
      ],
      "metadata": {
        "id": "Hj_6CI_UelPQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class SimpleDataset(Dataset):\n",
        "    def __init__(self, samples, labels):\n",
        "        self.samples = torch.tensor(samples, dtype=torch.float32) #convert samples to float tensor and assign them to self.samples\n",
        "        self.labels = torch.tensor(labels, dtype = torch.long)  # Convert labels to long tensor and assign to self.labels\n",
        "        self.n_samples = len(self.samples) # Store the number of samples in self.n_sample\n",
        "\n",
        "    def __len__(self):\n",
        "        # Return the total number of samples\n",
        "        return self.n_samples\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        return self.samples[index], self.labels[index]\n",
        "\n",
        "\n",
        "loader = SimpleDataset(train_samples_np,train_labels_np)\n",
        "print(loader[0])\n",
        "print(loader.n_samples)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4MS9vVKggKlI",
        "outputId": "4ab08604-b53a-4f97-8bbe-ca99c6900f7a"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(tensor(78.), tensor(1))\n",
            "20\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**3. Scaling the data**"
      ],
      "metadata": {
        "id": "cSUH2ug9ieYX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Rescale the samples to have a mean of 0 and a variance of 1\n",
        "scaler = StandardScaler()\n",
        "train_samples_scaled = scaler.fit_transform(train_samples_np.reshape(-1, 1))\n",
        "test_samples_scaled = scaler.transform(test_samples_np.reshape(-1, 1))\n",
        "\n",
        "# Create PyTorch Datasets\n",
        "train_dataset = SimpleDataset(train_samples_scaled, train_labels_np)\n",
        "test_dataset = SimpleDataset(test_samples_scaled, test_labels_np)\n",
        "\n",
        "# Create DataLoaders\n",
        "train_loader = DataLoader(train_dataset, batch_size=6, shuffle=True) # Create DataLoader with train_dataset, batch_size=6, shuffle=True\n",
        "test_loader = DataLoader(test_dataset, batch_size=5, shuffle=False)   # Create DataLoader with test_dataset, batch_size=5, shuffle=False"
      ],
      "metadata": {
        "id": "dTExZne1hSi1"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**4. Define the PyTorch Mode**"
      ],
      "metadata": {
        "id": "lqtShmoGiVS_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class SimpleNN(nn.Module):\n",
        "  def __init__(self, input_size, hidden_size, output_size):\n",
        "    super(SimpleNN, self).__init__()\n",
        "    self.hidden = nn.Linear(input_size, hidden_size)   # Define a Linear layer from input_size to hidden_size\n",
        "    self.sigmoid = nn.Sigmoid()  # Define a Sigmoid activation\n",
        "    self.output = nn.Linear(hidden_size, output_size)    # Define a Linear layer from hidden_size to output_size\n",
        "    self.softmax = nn.Softmax(dim=1)  # Define a Softmax activation along dimension 1\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = self.hidden(x)   # Apply the hidden layer\n",
        "    x = self.sigmoid(x)  # Apply the sigmoid activation\n",
        "    x = self.output(x)  # Pass through the output layer\n",
        "    x = self.softmax(x)  # Apply softmax to get output probabilities\n",
        "    return x\n",
        "\n",
        "\n",
        "# Instantiate the model\n",
        "input_size = 1\n",
        "hidden_size = 4\n",
        "output_size = 2 # Two output classes\n",
        "model = SimpleNN(input_size, hidden_size, output_size)"
      ],
      "metadata": {
        "id": "4lc9x5UxjVKD"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**5. Define Loss Function and Optimizer**"
      ],
      "metadata": {
        "id": "LyFBXZJrkpx-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "criterion = nn.CrossEntropyLoss()  # Define a loss function suitable for multi-class classification (e.g., CrossEntropyLoss)\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)  # Define an optimizer (e.g., SGD or Adam) with model parameters and learning rate"
      ],
      "metadata": {
        "id": "kjF5pXGvksQG"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**6. Train the Model**"
      ],
      "metadata": {
        "id": "QRj6zDnZkz7_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "num_epochs = 1000  # Set the number of training epochs\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    for inputs, labels in train_loader:\n",
        "        #print(inputs[0], labels[0])\n",
        "\n",
        "        # Zero the gradients\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Forward pass through the model to get outputs\n",
        "        outputs = model(inputs)\n",
        "# Compute the loss using criterion\n",
        "        loss = criterion(outputs, labels)\n",
        "\n",
        "        # Backward pass (loss.backward)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "\n",
        "    if (epoch + 1) % 50 == 0:\n",
        "        # Print epoch number and current loss\n",
        "        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')\n",
        "\n",
        "print(\"Finished Training\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8P2_P35Rk2gQ",
        "outputId": "9d6fbadf-0b6b-447b-c651-667b8f68f48e"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [50/1000], Loss: 0.5339\n",
            "Epoch [100/1000], Loss: 0.6880\n",
            "Epoch [150/1000], Loss: 0.6861\n",
            "Epoch [200/1000], Loss: 0.4972\n",
            "Epoch [250/1000], Loss: 0.4420\n",
            "Epoch [300/1000], Loss: 0.3935\n",
            "Epoch [350/1000], Loss: 0.4233\n",
            "Epoch [400/1000], Loss: 0.5222\n",
            "Epoch [450/1000], Loss: 0.4017\n",
            "Epoch [500/1000], Loss: 0.3659\n",
            "Epoch [550/1000], Loss: 0.3543\n",
            "Epoch [600/1000], Loss: 0.4676\n",
            "Epoch [650/1000], Loss: 0.3436\n",
            "Epoch [700/1000], Loss: 0.3245\n",
            "Epoch [750/1000], Loss: 0.3202\n",
            "Epoch [800/1000], Loss: 0.3269\n",
            "Epoch [850/1000], Loss: 0.3174\n",
            "Epoch [900/1000], Loss: 0.3679\n",
            "Epoch [950/1000], Loss: 0.3202\n",
            "Epoch [1000/1000], Loss: 0.3152\n",
            "Finished Training\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**7. Evaluate the Model**"
      ],
      "metadata": {
        "id": "q1x4coVbsZTm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Set the model to evaluation mode\n",
        "model.eval()\n",
        "all_predicted_labels = []\n",
        "all_test_labels = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for inputs, labels in test_loader:\n",
        "        outputs = model(inputs) # Perform a forward pass to get outputs\n",
        "\n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "        all_predicted_labels.extend(predicted.numpy())\n",
        "        all_test_labels.extend(labels.numpy())\n",
        "\n",
        "predicted_labels_np = np.array(all_predicted_labels)  # Convert all_predicted_labels to a NumPy array\n",
        "test_labels_np =  np.array(all_test_labels)       # Convert all_test_labels to a numpy array\n",
        "\n",
        "# Print predicted and true labels\n",
        "print(\"Predicted labels on testing set:\", predicted_labels_np)\n",
        "print(\"True labels on testing set:\", test_labels_np)\n",
        "\n",
        "# Compute prediction error as a percentage\n",
        "prediction_error_test = np.sum(np.abs(predicted_labels_np - test_labels_np)/len(test_labels_np))*100 # Compute the average absolute error percentage\n",
        "print(\"Prediction error on testing set:\", prediction_error_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gCzJmBPilx7E",
        "outputId": "daa2ff11-0247-43df-cc74-66264c0ae1d4"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predicted labels on testing set: [1 0 1 1 0 0 0 1 1 0 0 1 1 0 0 0 0 1 1 1]\n",
            "True labels on testing set: [1 0 1 1 0 0 0 1 1 0 0 1 1 0 0 0 0 1 1 1]\n",
            "Prediction error on testing set: 0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This notebook successfully implemented an assignment marker using a deep learning approach with PyTorch. The neural network, despite its simplicity, achieved perfect accuracy on the test set (0% prediction error), correctly classifying assignment marks as pass (1) or fail (0).\n",
        "\n",
        "Through this exercise, we explored critical concepts: data preprocessing with `StandardScaler`, custom dataset creation with `Dataset` and `DataLoader`, neural network design with `nn.Module`, and training with loss functions and optimizers. The model’s performance demonstrates that even a small network can learn simple patterns effectively, though its capacity far exceeds the needs of this binary classification task. For real-world applications, such complexity might be reserved for problems with more intricate data relationships.\n",
        "\n",
        "Happy Coding\n",
        "\n",
        "Joshua Robin: Author\n",
        "      "
      ],
      "metadata": {
        "id": "IZ2Jhsrcrj8z"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "1XctT8DesHyF"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}